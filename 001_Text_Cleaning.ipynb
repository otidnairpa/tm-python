{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "001_Text Cleaning",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhitology/tm-python/blob/master/001_Text_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyzyYArh3O2r",
        "colab_type": "text"
      },
      "source": [
        "*Muhammad Apriandito - Technaut Education*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITA7F96h3Jet",
        "colab_type": "text"
      },
      "source": [
        "# **Text Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfIO62ScWbXt",
        "colab_type": "text"
      },
      "source": [
        "Text preprocessing is traditionally an important step for natural language processing (NLP) tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UpC2JolU_jY",
        "colab_type": "text"
      },
      "source": [
        "## English Text Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhCoryR1V1i6",
        "colab_type": "text"
      },
      "source": [
        "Here we preprocess the simple word below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGSkSUS1Vzk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input English Text\n",
        "text_en = 'The death toll from the coronavirus has reached 28 in South Korea with 600 newly confirmed cases, raising the national Itally to 4,812 cases, the South Korean Centers for Disease Control and Prevention (KCDC) said in a news release Tuesday.'\n",
        "text_en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edMbDC4odWqs",
        "colab_type": "text"
      },
      "source": [
        "### **Remove Symbol and Character**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHpObEQaZH5B",
        "colab_type": "text"
      },
      "source": [
        "Remove symbol and character from the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtSNnHuAdb3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Library\n",
        "import string \n",
        "\n",
        "# Remove Symbol and Character\n",
        "text_en_nosymbol = text_en.translate(str.maketrans('','',string.punctuation)).lower()\n",
        "text_en_nosymbol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1_uwT7EVk8S",
        "colab_type": "text"
      },
      "source": [
        "###  **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRE7Yd8rWqSc",
        "colab_type": "text"
      },
      "source": [
        "Tokenization is the process of breaking a document down into words, punctuation marks, numeric digits, etc. We can do sentence tokenization and word tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHnq0MPJZbbZ",
        "colab_type": "text"
      },
      "source": [
        "#### **Sentence Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI9U7NG9U1TS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Module \n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Tokenize Sentence\n",
        "text_en_tokenizeds =sent_tokenize(text_en_nosymbol)\n",
        "\n",
        "# Show Tokenized Sentence\n",
        "text_en_tokenizeds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp7cNx3CZe6f",
        "colab_type": "text"
      },
      "source": [
        "#### **Word Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX-ym7GSVkjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word Tokenization\n",
        "\n",
        "# Import Module\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize Word\n",
        "text_en_tokenizedw = word_tokenize(text_en_nosymbol)\n",
        "\n",
        "# Show Tokenized Sentence\n",
        "text_en_tokenizedw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppeneYMaWb4P",
        "colab_type": "text"
      },
      "source": [
        "### **Remove Stopword**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg-j7uhaaPsW",
        "colab_type": "text"
      },
      "source": [
        "Though \"stop words\" usually refers to the most common words in a language. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as \"The Who\", \"The The\", or \"Take That\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6pW7G7wXEle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download English Stopwords from NLTK\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Show Stopwords\n",
        "stopwords_en = nltk.corpus.stopwords.words('english')\n",
        "stopwords_en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTOto8ctW3-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing Stopwords\n",
        "text_en_filtered =[]\n",
        "for w in text_en_tokenizedw:\n",
        "    if w not in stopwords_en:\n",
        "        text_en_filtered.append(w)\n",
        "\n",
        "# Show Tokenized vs Filtered\n",
        "print('Tokenized:',text_en_tokenizedw)\n",
        "print('Filtered:',text_en_filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3rd6uyoW_xP",
        "colab_type": "text"
      },
      "source": [
        "### **Text Normalization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-Lqx79cWwAF",
        "colab_type": "text"
      },
      "source": [
        "Text normalization considers another type of noise in the text. For example connection, connected, connecting word reduce to a common word \"connect\". It reduces derivationally related forms of a word to a common root word.\n",
        "\n",
        "Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfjzKWT7bf9S",
        "colab_type": "text"
      },
      "source": [
        "#### **Stemming**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecmB7mfnbhzW",
        "colab_type": "text"
      },
      "source": [
        "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root formâ€”generally a written word form. Example : consulting -> consult, parties -> parti"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBVajLjzXNlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Modules\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Set Stemming Function\n",
        "stemmer = PorterStemmer()\n",
        "text_en_stemmed =[]\n",
        "for i in text_en_filtered:\n",
        "    text_en_stemmed.append(stemmer.stem(i))\n",
        "\n",
        "# Show\n",
        "print('Filtered:',text_en_filtered)\n",
        "print('Stemmed:',text_en_stemmed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynqS0q87eN2W",
        "colab_type": "text"
      },
      "source": [
        "#### **Lemmzatization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oTTpa9eeCVR",
        "colab_type": "text"
      },
      "source": [
        "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. Example : best -> good, parti -> party"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rER-92If1gU",
        "colab_type": "text"
      },
      "source": [
        "*Lets try to compare stemming vs lemmatization!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31z9spWrXO3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Modules\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Try to the Words\n",
        "word = 'flying'\n",
        "print('Stemmed:',stemmer.stem(word))\n",
        "print('Lemmatized:',lemmatizer.lemmatize(word,'v'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4apnrXOgnq4",
        "colab_type": "text"
      },
      "source": [
        "*Lemmatizing the text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTuiiPgzYbJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Modules\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Set Lemmatization Function\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "text_en_lemmatized = [lemmatizer.lemmatize(i, pos='v') for i in text_en_filtered]\n",
        "\n",
        "# Show\n",
        "print('Filtered:',text_en_filtered)\n",
        "print('Stemmed:',text_en_stemmed)\n",
        "print('Lemmatized', text_en_lemmatized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhhRhfb8iRTM",
        "colab_type": "text"
      },
      "source": [
        "## **Indonesian Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNp-GXcTiZD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input Text (Indonesia)\n",
        "text_id = 'Rakyat memenuhi halaman gedung untuk menyuarakan isi hatinya kepada pemerintah pada tanggal 9 - maret - 2020.'\n",
        "text_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SP_84VmkPNF",
        "colab_type": "text"
      },
      "source": [
        "### **Remove Symbol and Character**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3fs7LR2izRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Library\n",
        "import string \n",
        "\n",
        "# Remove Symbol\n",
        "text_id_nosymbol = text_id.translate(str.maketrans('','',string.punctuation)).lower()\n",
        "text_id_nosymbol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRJ4-WQQkKM1",
        "colab_type": "text"
      },
      "source": [
        "### **Word Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04qa0z5qicTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Module\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize Word\n",
        "text_id_tokenized = word_tokenize(text_id_nosymbol)\n",
        "\n",
        "# Show\n",
        "text_id_tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS_VlVzPkULw",
        "colab_type": "text"
      },
      "source": [
        "### **Remove Stopword**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx5RJb6wjeo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download Indonesian Stopword\n",
        "stopwords_id = nltk.corpus.stopwords.words('indonesian')\n",
        "\n",
        "# Show Stopwords\n",
        "stopwords_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4zPeY7LjQGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing Stopword\n",
        "text_id_filtered = []\n",
        "for w in text_id_tokenized:\n",
        "    if w not in stopwords_id:\n",
        "        text_id_filtered.append(w)\n",
        "\n",
        "# Show\n",
        "print('Tokenized:',text_id_tokenized)\n",
        "print('Filtered:',text_id_filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE_s53fxkjrd",
        "colab_type": "text"
      },
      "source": [
        "### **Text Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqPXepEyJ66y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install Library for Text Normalization\n",
        "! pip install sastrawi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeRIjvSDKCW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Library\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr2MTopokDr2",
        "colab_type": "text"
      },
      "source": [
        "#### **Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZFKF1PeKGK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Module\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "# Create Stemmer Function\n",
        "stemmer_id = StemmerFactory().create_stemmer()\n",
        "text_id_stemmed = [stemmer_id.stem(i) for i in text_id_filtered]\n",
        "\n",
        "# Show\n",
        "print('Filtered:', text_id_filtered)\n",
        "print('Stemmed:', text_id_stemmed)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}